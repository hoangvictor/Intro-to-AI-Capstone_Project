{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8damvcxCA7a"
   },
   "source": [
    "# **1. Problem Description and Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vVqxjTiH5sc"
   },
   "source": [
    "## **1.1. Problem description:**\n",
    "\n",
    "Given a grayscale topographic map of a part of Mars and some features of the land rover.\n",
    "\n",
    "Assume that:\n",
    "\n",
    "+ The land rover knows the starting point and the ending point, it has also an elevation map in which the height is represented by 254 pixels (2 for the deepest position and 255 for the highest position) and the sensors to know where it is. $(1)$\n",
    "\n",
    "+ The land rover can move in only 4 directions: forward, backward, left and right, it can go up or down with an inclination of $\\theta \\leq 10^{\\circ}$ (this value can be computed by the formula: $tan(\\theta) = \\dfrac{|h_1 - h_2|}{dist(P_1, P_2)}$ which is the ratio between the altitude difference of $P_1$ and $P_2$ and their real distance ($P_1$ and $P_2$ are 2 consecutive positions, therefore ${dist(P_1, P_2)} = const$). $(2)$\n",
    "\n",
    "We need to find the shortest path (if it exists) for the land rover to travel between 2 points from the starting point A and the ending point B. The goal is the destination point B.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFovuxaXggmF"
   },
   "source": [
    "## **1.2. Data Preparation:**\n",
    "\n",
    "**Import the topography map image from the Internet:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"Mars_MGS_MOLA_DEM.jpg\" #The name of the file we want to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mntX7CdgvBE"
   },
   "outputs": [],
   "source": [
    "# DON'T RUN THIS CELL IF YOU ALREADY HAVE THE IMAGE\n",
    "import requests #Module for getting image from the Internet\n",
    "response = requests.get(\"http://planetpixelemporium.com/download/download.php?5672/mars_12k_topo.jpg\")\n",
    "\n",
    "with open(file_name, \"wb\") as file: #Create a file with the name above and save the content of response to it\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJTvzPLZlhMO"
   },
   "source": [
    "**Import modules:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmvN-96aCCpw"
   },
   "outputs": [],
   "source": [
    "from PIL import Image # work with images\n",
    "import matplotlib.pyplot as plt # work with graph\n",
    "import numpy as np # work with arrays\n",
    "import time # for analysis process\n",
    "import queue # queue data structure\n",
    "import pickle # for saving object\n",
    "import os \n",
    "import pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fe5DYJAgM0Yt"
   },
   "source": [
    "This is Mars Orbiter Laser Altimeter **(MOLA)** map of Mars. According to NASA, \"MOLA is an instrument on the Mars Global Surveyor (MGS), a spacecraft that was launched on November 7, 1996. The mission of MGS was to orbit Mars, and map it over the course of approximately 3 years, which it did sucessfully, completing 4 1/2 years of mapping.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HQYrhCY5h80k",
    "outputId": "bd4142ac-cf58-4899-8a74-33efa33b75b8"
   },
   "outputs": [],
   "source": [
    "img = Image.open(file_name) #Open our saved file above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOFOpXAWGLTM"
   },
   "source": [
    "**Image description:**\n",
    "\n",
    "As we can see in the image, the darker region means that the region is deeper, the lighter areas mean higher altitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "FiBVoMICimmt",
    "outputId": "fda1a971-ae74-494e-a4c7-ad3303351f1c"
   },
   "outputs": [],
   "source": [
    "print(\"Shape of original image: \", np.asarray(img).shape)\n",
    "img = img.convert(\"L\") #Convert to one channel grey image\n",
    "img_array = np.asarray(img)  #Convert the image to an array\n",
    "img_array = img_array.astype('int32')\n",
    "print(\"Shape of converted image: \", img_array.shape)\n",
    "print(\"Image_array[0:2]: \", *img_array[0:2]) \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9), dpi = 120) #Figure setting for plotting\n",
    "plt.axis('off') #Turn off axis\n",
    "plt.imshow(img_array, cmap = 'gray') #Show the gray figure\n",
    "plt.title(\"MOLA Mars map 12K\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82ncRRHaGzUk"
   },
   "source": [
    "**Mars information:**\n",
    "\n",
    "According to NASA, the maximum altitude is 21229 m (peak of Olympus Mons) and the minimum altitude is -8200m (Hellas Impact Crater). \n",
    "\n",
    "The pixel resolution of this map is 1736.25 meters per pixel (m). So $dist(P_1,P_2) = const = 1736.25 (m)$ ($P_1$ and $P_2$ are 2 consecutive positions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JA6hqKU1EB4x"
   },
   "source": [
    "**Topography map information:**\n",
    "\n",
    "According to https://homepages.inf.ed.ac.uk/rbf/HIPR2/value.htm, we need to note that for a grayscale image, the pixel value is a single number that represents the brightness of the pixel. The most common pixel format is the byte image, where this number is stored as an 8-bit integer giving a range of possible values from 0 to 255. Typically zero is taken to be black, and 255 is taken to be white.\n",
    "\n",
    "For this particular MOLA Mars map image above, we found that the maximum height (which is equivalent to the peak of Olympus Mons) is represented by a pixel value of 255 and the minimum height (which is equivalent to the Hellas Impact Crater) is represented by a pixel value of 2 (this is the reason why in part $(1)$ of the problem description, we assume that 2 is the pixel value representing for the deepest position and 255 for the highest position)\n",
    "\n",
    "Therefore, each value pixel represents for: $\\dfrac{1}{255 - 2 + 1} \\times (max_{height} - min_{height}) = \\dfrac{1}{254} \\times (21229 + 8200) = 115.86$ meters of height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhV4iWqPH4dU"
   },
   "source": [
    "**Random picking:**\n",
    "+ The starting point\n",
    "+ The destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 875
    },
    "id": "jnGjzoyOJB9d",
    "outputId": "3700ca1e-b347-40c6-8d3e-760bc4dde129"
   },
   "outputs": [],
   "source": [
    "start = (np.random.randint(12288), np.random.randint(6144))\n",
    "end = (np.random.randint(12288), np.random.randint(6144))\n",
    "print(\"Starting point: \", start, \"Ending point: \", end)\n",
    "print()\n",
    "plt.figure(figsize = (16,9), dpi = 120) #Figure setting for plotting\n",
    "plt.imshow(img_array, cmap = 'gray') #Show the gray figure\n",
    "plt.scatter(*start) #Plot the starting point\n",
    "plt.scatter(*end) #Plot the ending point\n",
    "plt.title(\"MOLA Mars map\")\n",
    "plt.legend([\"Starting point\", \"Ending point\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1WVF8pLIsHp"
   },
   "source": [
    "We need to find the shortest path (if it exists) for the land rover to travel between 2 points from the starting point A and the ending point B, but our path needs to satisfy $(2)$. Let's take an example of 2 consecutive points that our land rover can't move between them.\n",
    "\n",
    "Take 2 points $P_1$ and $P_2$ next to each other:\n",
    "\n",
    "+ P1: img_array[2000,5085] = 75\n",
    "+ P2: img_array[2000,5086] = 78\n",
    "\n",
    "Since the difference between the pixel value of these 2 points is 3, so in reality, the difference between the elevation of these 2 points is $3 \\times 115.86 = 347.58 (m)$. Others, because $dist(P_1,P_2) = const = 1736.25 (m)$. Therefore $\\theta = arctan\\left(\\dfrac{347.58}{1736.25}\\right) = 11.32 ^{\\circ} > 10 ^{\\circ}$. In conclusion, the land rover can't go from $P_1$ to $P_2$ or vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ipng8WQ_mIw"
   },
   "source": [
    "# 2. Analysis Plan\n",
    "\n",
    "In order to solve this problem, we consider:\n",
    "\n",
    "+ 3 algorithms (1 uninformed search algorithms and 2 informed search algorithms): Uniform cost search, A* search, Greedy best first search.\n",
    "\n",
    "+ 2 data structures with different sort algorithms for frontier: Priority queue (with heap sort) and List (with timsort)\n",
    "\n",
    "+ 2 approaches for finding the neighbors of each point: Create the list of neighborhood for each node first and put it in an array before running the algorithm; Check the available neighbors for each node while running the algorithm.\n",
    "\n",
    "+ 5 types of heuristics: using Manhattan distance; Euclidean distance; Octile distance; Tie-Breaking Low g-cost; Tie-Breaking High g-cost.\n",
    "\n",
    "Others, since the original image has a very big size $(6144 \\times 12288)$, therefore we consider only the part of this image with different size $15 \\times 15, 20 \\times 20, 25 \\times 25, 30 \\times 30)$.\n",
    "\n",
    "We hypothesize also that the running time of each algorithm depends also on the standard deviation of pixel value for each value, therefore we need to divide the original image into many small images belonging to each bin of standard deviation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tx5W1WGh_mnd"
   },
   "outputs": [],
   "source": [
    "img_size = [15, 20, 25, 30, 50, 75, 100] # All the image size that we consider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QScsykEACeY"
   },
   "source": [
    "# 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "id": "THcaGpkUAAlT",
    "outputId": "66c2e555-aa76-4fa8-de39-1c64855ac965"
   },
   "outputs": [],
   "source": [
    "x_max, y_max = img_array.shape\n",
    "print(\"x_max: {}, y_max: {}\".format(x_max, y_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ra5177yLAEZg"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Arrays[size] stores the data of images having specific size\n",
    "arrays = {}\n",
    "\n",
    "# We show only the case where img_size = 50, for the remaining cases, we do \n",
    "# the same analysis as below and we got the similar results\n",
    "size = img_size[4] \n",
    "\n",
    "# Create a list for storing a tuple of image and its standard deviation (section, np.std(section))\n",
    "arrays[size] = []\n",
    "\n",
    "# Generate data\n",
    "for i in range(0, x_max - size + 1, size//5): # Step = size//5\n",
    "    for j in range(0, y_max - size + 1, size//5): # Step = size//5\n",
    "        section = img_array[i:i + size, j:j + size] # Take the section of image\n",
    "        arrays[size].append((section, np.std(section))) # Append (section, standard_deviation(section)) to arrays[size]\n",
    "\n",
    "#Sort the image sections according to the standard deviation\n",
    "arrays[size] = sorted(arrays[size], key = lambda tup: tup[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVZjLxwpAJIg"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Store all the standard deviations to std_data\n",
    "std_data = np.array([x[1] for x in arrays[size]])\n",
    "\n",
    "# Plot all the data in std_data\n",
    "plt.figure(figsize = (16, 9), dpi = 80)\n",
    "plt.hist(std_data, bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEVy_yiUALDn"
   },
   "source": [
    "For images of size $15$, we can see that the standard deviation of each small images is near by 0. Let's consider only the case where the standard deviation of an image is less than $8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNemRkYYAMN4"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Crop the std_data and save it to std_data_crop, we just consider the case where the standard deviation which is less than 8\n",
    "std_data_crop = np.array([x for x in std_data if x < 8])\n",
    "\n",
    "# Plot all the data in std_data\n",
    "plt.figure(figsize = (16, 9), dpi = 80)\n",
    "plt.hist(std_data_crop, bins = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cg_HSCaRAN24"
   },
   "source": [
    "From the histogram above, we can see that, most standard deviations are between $0$ and $1$. In order to discover the effect of standard deviations on the algorithm running time, we take the data in the first $16$ bins (each bin has size $0.5$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHu4puvJBTuf"
   },
   "source": [
    "# 4. Analysis\n",
    "\n",
    "3 algorithms (1 uninformed search algorithms and 2 informed search algorithms): Uniform cost search, A* search, Greedy best first search.\n",
    "\n",
    "2 data structures with different sort algorithms for frontier: Priority queue (with heap sort) and List (with timsort)\n",
    "\n",
    "2 approaches for finding the neighbors of each point: Create the list of neighborhood for each node first and put it in an array before running the algorithm; Check the available neighbors for each node while running the algorithm.\n",
    "\n",
    "5 types of heuristics: using Manhattan distance; Tie-Breaking High g-cost, Variance of Tie-Breaking High g-cost, Tie-Breaking Low g-cost, Variance of Tie-Breaking Low g-cost.\n",
    "\n",
    "# 4.1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data\n",
    "+ For $img_{size} = 15$, we consider only the case where the standard deviation of an image is less than $8$ and we choose also the first 16 bins of size 0.5.\n",
    "\n",
    "+ For $img_{size} = 20$, we consider only the case where the standard deviation of an image is less than $8$ and we choose also the first 16 bins of size 0.5.\n",
    "\n",
    "+ For $img_{size} = 25$, we consider only the case where the standard deviation of an image is less than $8$ and we choose also the first 16 bins of size 0.5.\n",
    "\n",
    "+ For $img_{size} = 30$, we consider only the case where the standard deviation of an image is less than $8$ and we choose also the first 16 bins of size 0.5.\n",
    "\n",
    "+ For $img_{size} = 50$, we consider only the case where the standard deviation of an image is less than $8$ and we choose also the first 16 bins of size 0.5.\n",
    "\n",
    "+ For $img_{size} = 75$, we consider only the case where the standard deviation of an image is less than $8$ and we choose also the first 16 bins of size 0.5.\n",
    "\n",
    "+ For $img_{size} = 100$, we consider only the case where the standard deviation of an image is less than $8$ and we choose also the first 16 bins of size 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import random \n",
    "\n",
    "#Variables\n",
    "arrays = {}\n",
    "data = {}\n",
    "\n",
    "#Bin information\n",
    "num_bins = 16\n",
    "bin_size = 0.5\n",
    "\n",
    "for size in img_size:\n",
    "    arrays[size] = []\n",
    "    for i in range(0, x_max - size + 1, size//5):\n",
    "        for j in range(0, y_max - size + 1, size//5):\n",
    "            section = img_array[i:i + size, j:j + size]\n",
    "            arrays[size].append((section, np.std(section)))\n",
    "\n",
    "for size in img_size:\n",
    "    tmp_data = {}\n",
    "    \n",
    "    i = bin_size\n",
    "    \n",
    "    while i <= num_bins * bin_size:\n",
    "        \n",
    "        #Standard deviation filtering\n",
    "        tmp_data[(i-bin_size,i)] = [arr[0] for arr in arrays[size] if arr[1] < i] \n",
    "        \n",
    "        #Randomize our data before writing it to a txt file\n",
    "        random.shuffle(tmp_data[(i-bin_size,i)]) \n",
    "        \n",
    "        #Save only 100 random images for analysis process -> Change 1 to 100 for real analysiS\n",
    "        tmp_data[(i-bin_size,i)] = tmp_data[(i-bin_size,i)][0:100] # Take only 1 image for fast video capture  \n",
    "        \n",
    "        i += bin_size\n",
    "    \n",
    "    data[size] = tmp_data\n",
    "\n",
    "# Long running time for this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXePHaR5CQZX"
   },
   "source": [
    "Path for storing our analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iu8zzzCQBUCE"
   },
   "outputs": [],
   "source": [
    "path = \"Analysis Results\"\n",
    "\n",
    "# Make folder if path does not exist\n",
    "if os.path.exists(path) == False:\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59dlLqYyCVTY"
   },
   "source": [
    "Heuristic functions (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7L2N-yCjBmNX"
   },
   "outputs": [],
   "source": [
    "# Manhattan distance\n",
    "def manhattan(actual_node, start_node ,goal_node, delta = None):\n",
    "    actualPos = actual_node.position\n",
    "    endPos = goal_node.position\n",
    "    return abs(actualPos[0] - endPos[0]) + abs(actualPos[1] - endPos[1])\n",
    "\n",
    "# There are many states with the same f-cost, and we have to choose the order in which to expand them. \n",
    "# For tie_breaking_high_g_cost, we preferred states closer to the goal node than the goal state.\n",
    "def tie_breaking_high_g_cost(actual_node, start_node, goal_node, delta = 0.001):\n",
    "    actualPos = actual_node.position\n",
    "    endPos = goal_node.position\n",
    "    return manhattan(actual_node, start_node, goal_node) * (1 + delta)\n",
    "\n",
    "# Variance of tie breaking high g-cost, we vary the value of delta in tie_breaking_high_g_cost, according\n",
    "# to the position of the actual node\n",
    "def var_tie_breaking_high_g_cost(actual_node, start_node, goal_node, delta = 0.001):\n",
    "    actualPos = actual_node.position\n",
    "    endPos = goal_node.position\n",
    "    rate1 = manhattan(actual_node, start_node, goal_node)\n",
    "    rate2 = manhattan(start_node, start_node, goal_node)\n",
    "    return rate1 * (1 + delta* (0.5 + rate1/rate2))\n",
    "\n",
    "# There are many states with the same f-cost, and we have to choose the order in which to expand them. \n",
    "# For tie_breaking_low_g_cost, we preferred states closer to the start node than the goal state.\n",
    "def tie_breaking_low_g_cost(actual_node, start_node, goal_node, delta = 0.001):\n",
    "    actualPos = actual_node.position\n",
    "    endPos = goal_node.position\n",
    "    return manhattan(actual_node, start_node, goal_node) * (1 - delta)\n",
    "\n",
    "# Variance of tie breaking low g-cost, we vary the value of delta in tie_breaking_low_g_cost, according\n",
    "# to the position of the actual node\n",
    "def var_tie_breaking_low_g_cost(actual_node, start_node, goal_node, delta = 0.001):\n",
    "    actualPos = actual_node.position\n",
    "    endPos = goal_node.position\n",
    "    rate1 = manhattan(actual_node, start_node, goal_node)\n",
    "    rate2 = manhattan(start_node, start_node, goal_node)\n",
    "    return rate1 * (1 + delta * (1.5 - rate1/rate2))\n",
    "\n",
    "# Heuristics dictionary containing all kinds of heuristic functions\n",
    "heuristics = {\n",
    "    'manhattan': manhattan,\n",
    "    'tie_breaking_high_g_cost': tie_breaking_high_g_cost,\n",
    "    'var_tie_breaking_high_g_cost': var_tie_breaking_high_g_cost,\n",
    "    'tie_breaking_low_g_cost': tie_breaking_low_g_cost,\n",
    "    'var_tie_breaking_low_g_cost': var_tie_breaking_low_g_cost\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3XPODbiCYpO"
   },
   "source": [
    "Node definition, Analysis object, Methods of finding neighbors, Data initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Keg3cJIfBpJm"
   },
   "outputs": [],
   "source": [
    "#------------------------------------------ Node definition -----------------------------------------\n",
    "class Node():\n",
    "    # Node initialization\n",
    "    def __init__(self, position):\n",
    "        self.position = position # Tuple (x,y)\n",
    "        self.g = 0 # g-cost\n",
    "        self.h = 0 # h-cost\n",
    "        self.f = None # f-cost\n",
    "        self.parent = None # Parent node\n",
    "    \n",
    "    # Print the node\n",
    "    def __str__(self): \n",
    "        return str(self.position)\n",
    "    \n",
    "    # Equality of 2 nodes (position)\n",
    "    def __eq__(self, other):\n",
    "        return self.position == other.position\n",
    "    \n",
    "    # Less than\n",
    "    def __lt__(self, other):\n",
    "        return self.f < other.f\n",
    "    \n",
    "    # Greater than\n",
    "    def __gt__(self, other):\n",
    "        return self.f > other.f\n",
    "\n",
    "\n",
    "#-------------------------------------- Analysis initialization -------------------------------------\n",
    "# Define the class containing our analysis results\n",
    "class AnalysisResults:\n",
    "    def __init__(self, size):\n",
    "        self.size = size # Size of image\n",
    "        \n",
    "        # Dictionary of average run time and average steps_count, for example: \n",
    "        # avg_run_time[(0,0.5)] returns the average run time of image inside the \n",
    "        # bin of standard deviation from 0 to 0.5\n",
    "        self.avg_run_time = {}\n",
    "        self.avg_steps_count = {}\n",
    "        \n",
    "        \n",
    "#----------------------------------------- Finding neighbors ----------------------------------------\n",
    "# Finding while runing\n",
    "def next_pos_list(array, actual_node, img_shape, theta):\n",
    "    '''\n",
    "    array: array of height value\n",
    "    actual_node: node object of the actual node that you want to find its neighbors\n",
    "    img_shape: tuple(x_size, y_size) - size of the image\n",
    "    theta: maximum distance of heigh between 2 consecutive positions\n",
    "    '''\n",
    "    res = []\n",
    "    x_size, y_size = img_shape\n",
    "    actualPos = actual_node.position\n",
    "    actual_nodeValue = array[actualPos[0]][actualPos[1]]\n",
    "    \n",
    "    # Conditions checking for neighbors\n",
    "    if actualPos[0] + 1 < x_size and abs(actual_nodeValue - array[actualPos[0] + 1][actualPos[1]]) <= theta:\n",
    "        res.append((actualPos[0] + 1, actualPos[1]))\n",
    "    if actualPos[1] + 1 < y_size and abs(actual_nodeValue - array[actualPos[0]][actualPos[1] + 1]) <= theta:\n",
    "        res.append((actualPos[0], actualPos[1] + 1))\n",
    "    if actualPos[0] - 1 >= 0 and abs(actual_nodeValue - array[actualPos[0] - 1][actualPos[1]]) <= theta:\n",
    "        res.append((actualPos[0] - 1, actualPos[1]))\n",
    "    if actualPos[1] - 1 >= 0 and abs(actual_nodeValue - array[actualPos[0]][actualPos[1] - 1]) <= theta:\n",
    "        res.append((actualPos[0], actualPos[1] - 1))\n",
    "    \n",
    "    return res\n",
    "       \n",
    "    \n",
    "# Find neighbors first before run\n",
    "def make_grid(array, img_shape, theta):\n",
    "    '''\n",
    "    array: array of height value\n",
    "    img_shape: tuple(x_size, y_size) - size of the image\n",
    "    theta: maximum distance of heigh between 2 consecutive positions\n",
    "    '''\n",
    "    grid = [[None for j in range(len(array))] for i in range(len(array))]\n",
    "    x_size, y_size = img_shape\n",
    "\n",
    "    for i in range(len(array)):\n",
    "        for j in range(len(array)):\n",
    "            res = []\n",
    "            actual_nodeValue = int(array[i][j])  \n",
    "            \n",
    "            # Conditions checking for neighbors\n",
    "            if i + 1 < x_size and abs(actual_nodeValue - int(array[i + 1][j])) <= theta:\n",
    "                res.append((i + 1,j))\n",
    "            if j + 1 < y_size and abs(actual_nodeValue - int(array[i][j + 1])) <= theta:\n",
    "                res.append((i,j + 1))\n",
    "            if i - 1 >= 0 and abs(actual_nodeValue - int(array[i - 1][j])) <= theta:\n",
    "                res.append((i - 1,j))\n",
    "            if j - 1 >= 0 and abs(actual_nodeValue - int(array[i][j - 1])) <= theta:\n",
    "                res.append((i,j - 1))\n",
    "            grid[i][j] = res # grid[i][j] contains a list of points which can be the neighbors of the point at position (i,j)\n",
    "    \n",
    "    return grid\n",
    "\n",
    "#----------------------------------------- Data initialization ----------------------------------------\n",
    "# Random the position of node\n",
    "def init(size):\n",
    "    start_node = (np.random.randint(0,size), np.random.randint(0,size))\n",
    "    goal_node = (np.random.randint(0,size), np.random.randint(0,size))\n",
    "    return start_node, goal_node\n",
    "\n",
    "# Create a list of n pairs of nodes\n",
    "def random_initialization(n, size):\n",
    "    return [init(size) for i in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b29t82kdBrOP"
   },
   "source": [
    "### 4.1.1. Astar analysis function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMkesXnuCpgR"
   },
   "outputs": [],
   "source": [
    "def Astar_analysis(data, parameters):        \n",
    "    #-------------------------------------- 3 different A* algorithms --------------------------------------\n",
    "    \n",
    "    # Using list (with timsort) for frontier, finding neighbors while running\n",
    "    def norm_Astar_list(img_array, start_node, goal_node, img_shape, theta, heuristic):\n",
    "        \n",
    "        steps_count = 0 # Save our number of steps\n",
    "        visited_nodes = set() # Save our visited nodes\n",
    "        frontier = [start_node] # Add the start_node to the frontier\n",
    "\n",
    "        while len(frontier) > 0: # While there is something inside the frontier, continue the algorithm\n",
    "            \n",
    "            # Sort the frontier in reverse order, here Python uses timsort\n",
    "            frontier = sorted(frontier, reverse = True)\n",
    "            \n",
    "            # Pop the last node, which has the lowest value of f\n",
    "            actual_node = frontier.pop()\n",
    "            \n",
    "            # Add 1 to steps_count for each time we enter a node\n",
    "            steps_count += 1 \n",
    "            \n",
    "            # Save the temporary g-score of the actual node\n",
    "            tmp_g_score = actual_node.g\n",
    "            \n",
    "            # Return the steps_count for analysis process\n",
    "            if actual_node == goal_node:\n",
    "                return steps_count\n",
    "            \n",
    "            # Add the position (tuple(x,y)) to visited_node\n",
    "            visited_nodes.add(actual_node.position)\n",
    "            \n",
    "            # Create a list of temporary visited nodes, add these nodes to the frontier after the for loop below \n",
    "            tmp_visited_nodes = []\n",
    "            \n",
    "            # Iterate over all neighbors of the actual node\n",
    "            for next_pos in next_pos_list(img_array, actual_node, img_shape, theta):\n",
    "                steps_count += 1 # Add 1 for each time we check the next node\n",
    "                next_node = Node(next_pos) # Create next_node with position next_pos\n",
    "                \n",
    "                # If next_pos in visited_nodes, we don't do anything\n",
    "                if next_pos not in visited_nodes:\n",
    "\n",
    "                    # f = h + g, update f-score\n",
    "                    next_node.g = tmp_g_score + 1\n",
    "                    next_node.h = heuristic(next_node, start_node, goal_node, delta) \n",
    "                    next_node.f = next_node.h + next_node.g\n",
    "                    next_node.parent = actual_node\n",
    "                    \n",
    "                    # If next_node in frontier\n",
    "                    if next_node in frontier:\n",
    "                        idx = frontier.index(next_node) # Find the index of next_node in the frontier\n",
    "                        \n",
    "                        # If the f-score of the duplicate node is more than that of the actual next_node\n",
    "                        if frontier[idx] > next_node: \n",
    "                            frontier.remove(next_node) # Remove this next_node in the frontier\n",
    "                            tmp_visited_nodes.append(next_node) # And append the actual next_node to the frontier\n",
    "                    else:\n",
    "                        tmp_visited_nodes.append(next_node) # Append the actual next_node to the frontier\n",
    "            \n",
    "            # Add all the nodes inside tmp_visited_nodes into frontier\n",
    "            frontier += tmp_visited_nodes\n",
    "        \n",
    "        # Return the number of steps counted for analysis process\n",
    "        return steps_count\n",
    "    \n",
    "    \n",
    "    # Using priority queue (with heapsort) for frontier, finding neighbors while running\n",
    "    def norm_Astar_priority_queue(img_array, start_node, goal_node, img_shape, theta, heuristic):\n",
    "        steps_count = 0 # Save our number of steps\n",
    "        visited_nodes = set() # Save our visited nodes\n",
    "        frontier = queue.PriorityQueue()\n",
    "        frontier.put((0, start_node))\n",
    "\n",
    "        while frontier.empty() == False:\n",
    "            actual_node = frontier.get()[1]        \n",
    "            steps_count += 1 # Add 1 for each time we enter a node\n",
    "            tmp_g_score = actual_node.g\n",
    "\n",
    "            if actual_node == goal_node:\n",
    "                return steps_count\n",
    "\n",
    "            visited_nodes.add(actual_node.position)\n",
    "\n",
    "            # Iterate over all neighbors of the actual node\n",
    "            for next_pos in next_pos_list(img_array, actual_node, img_shape, theta):\n",
    "                steps_count += 1 # Add 1 for each time we check the next node\n",
    "                next_node = Node(next_pos) # Create next_node with position next_pos\n",
    "                \n",
    "                # If next_pos in visited_nodes, we don't do anything\n",
    "                if next_pos not in visited_nodes:\n",
    "\n",
    "                    # f = h + g, update f-score\n",
    "                    next_node.g = tmp_g_score + 1\n",
    "                    next_node.h = heuristic(next_node, start_node, goal_node, delta) \n",
    "                    next_node.f = next_node.h + next_node.g\n",
    "                    next_node.parent = actual_node\n",
    "\n",
    "                    # Check if there exists a node having the same position as next_node (denoted as duplicate node) \n",
    "                    # in the frontier or not\n",
    "                    if (next_node.f, next_node) in frontier.queue:\n",
    "                        # Find the index of the duplicate node in the frontier.queue\n",
    "                        idx = frontier.queue.index((next_node.f, next_node)) \n",
    "\n",
    "                        # Check the f-value of the 2 nodes (next_node and the duplicate node in the frontier)\n",
    "                        if frontier.queue[idx][0] > next_node.f:\n",
    "                            \n",
    "                            # Replace the duplicate node by the next_node with lower f-score\n",
    "                            # Step 1: Create a temporary queue\n",
    "                            tmp_queue = queue.PriorityQueue()\n",
    "                            \n",
    "                            # Step 2: Put all the elements of the frontier to tmp_queue until we find the duplicate node\n",
    "                            while t != frontier.queue[idx]:\n",
    "                                t = frontier.get()\n",
    "                                tmp_queue.put(t)\n",
    "                            \n",
    "                            # Step 3: Delete the duplicate node\n",
    "                            t = frontier.get()\n",
    "                            \n",
    "                            # Step 4: Reput all the elements from the tmp_queue to the frontier\n",
    "                            while tmp_queue.empty() == False:\n",
    "                                t = tmp_queue.get()\n",
    "                                frontier.put(t)    \n",
    "                                \n",
    "                            frontier.put((next_node.f, next_node)) # Put next_node to frontier\n",
    "                    else:\n",
    "                        frontier.put((next_node.f, next_node)) # Put next_node to frontier\n",
    "        \n",
    "        # Return the number of steps counted for analysis process\n",
    "        return steps_count\n",
    "    \n",
    "    \n",
    "     # Using list (with timsort) for frontier, finding neighbors before running\n",
    "    def Astar_list_with_init(grid, img_array, start_node, goal_node, img_shape, theta, heuristic):    \n",
    "        steps_count = 0 # Save our number of steps\n",
    "        visited_nodes = set() # Save our visited nodes\n",
    "        frontier = [start_node] # Add the start_node to the frontier\n",
    "\n",
    "        while len(frontier) > 0: # While there is something inside the frontier, continue the algorithm\n",
    "            \n",
    "            # Sort the frontier in reverse order, here Python uses timsort\n",
    "            frontier = sorted(frontier, reverse = True)\n",
    "            \n",
    "            # Pop the last node, which has the lowest value of f\n",
    "            actual_node = frontier.pop()\n",
    "            \n",
    "            # Add 1 to steps_count for each time we enter a node\n",
    "            steps_count += 1 \n",
    "            \n",
    "            # Save the temporary g-score of the actual node\n",
    "            tmp_g_score = actual_node.g\n",
    "            \n",
    "            # Return the steps_count for analysis process\n",
    "            if actual_node == goal_node:\n",
    "                return steps_count\n",
    "            \n",
    "            # Add the position (tuple(x,y)) to visited_node\n",
    "            visited_nodes.add(actual_node.position)\n",
    "            \n",
    "            # Create a list of temporary visited nodes, add these nodes to the frontier after the for loop below \n",
    "            tmp_visited_nodes = []\n",
    "\n",
    "            # Iterate over all neighbors of the actual node\n",
    "            for next_pos in grid[actual_node.position[0]][actual_node.position[1]]:\n",
    "                steps_count += 1 # Add 1 for each time we check the next node\n",
    "                next_node = Node(next_pos) # Create next_node with position next_pos\n",
    "                \n",
    "                # If next_pos in visited_nodes, we don't do anything\n",
    "                if next_pos not in visited_nodes:\n",
    "\n",
    "                    # f = h + g, update f-score\n",
    "                    next_node.g = tmp_g_score + 1\n",
    "                    next_node.h = heuristic(next_node, start_node, goal_node, delta) \n",
    "                    next_node.f = next_node.h + next_node.g\n",
    "                    next_node.parent = actual_node\n",
    "\n",
    "                    # If next_node in frontier\n",
    "                    if next_node in frontier:\n",
    "                        idx = frontier.index(next_node) # Find the index of next_node in the frontier\n",
    "                        \n",
    "                        # If the f-score of the duplicate node is more than that of the actual next_node\n",
    "                        if frontier[idx] > next_node: \n",
    "                            frontier.remove(next_node) # Remove this next_node in the frontier\n",
    "                            tmp_visited_nodes.append(next_node) # And append the actual next_node to the frontier\n",
    "                    else:\n",
    "                        tmp_visited_nodes.append(next_node) # Append the actual next_node to the frontier\n",
    "            \n",
    "            # Add all the nodes inside tmp_visited_nodes into frontier\n",
    "            frontier += tmp_visited_nodes\n",
    "        \n",
    "        # Return the number of steps counted for analysis process\n",
    "        return steps_count    \n",
    "    \n",
    "    #------------------------------------ Retrieving data from parameters ------------------------------------\n",
    "    \n",
    "    #List of image size:\n",
    "    img_size = parameters['img_size']\n",
    "    \n",
    "    #Type of frontier (list or priority_queue)\n",
    "    frontier_type = parameters['frontier_type']    \n",
    "    \n",
    "    #Type of finding neighbors (find_first_before_run or find_while_running)\n",
    "    finding_neighbors = parameters['finding_neighbors']\n",
    "    \n",
    "    #Choose Astar version\n",
    "    if finding_neighbors == 'find_while_running' and frontier_type == 'list':\n",
    "        Astar = norm_Astar_list \n",
    "    elif finding_neighbors == 'find_while_running' and frontier_type == 'priority_queue':\n",
    "        Astar = norm_Astar_priority_queue\n",
    "    elif finding_neighbors == 'find_first_before_run' and frontier_type == 'list':\n",
    "        Astar = Astar_list_with_init\n",
    "    \n",
    "    #Type of heuristic function (manhattan; tie_breaking_high_g_cost)\n",
    "    global heuristics\n",
    "    heuristic_type = parameters['heuristic_type'] \n",
    "    heuristic = heuristics[heuristic_type]\n",
    "    \n",
    "    #Delta for heuristic functions\n",
    "    delta = 0.001\n",
    "    if 'delta' in parameters.keys():\n",
    "        delta = parameters['delta']\n",
    "\n",
    "    #Number of random starting points and ending points to run analysis\n",
    "    num_points = parameters['num_points'] \n",
    "    \n",
    "    \n",
    "    #-------------------------------------------- Analysis process --------------------------------------------\n",
    "    # Store the analysis results\n",
    "    analysis_results = []\n",
    "    \n",
    "    # Iterate over all the image size in data.keys()\n",
    "    for size in data.keys():\n",
    "        \n",
    "        # Preparation\n",
    "        analysis_results.append(AnalysisResults(size)) # Create a new AnalysisResults object and append it to the list\n",
    "        pair_node_list = random_initialization(num_points, size) # Create a random pair of start node and goal node\n",
    "        print(\"Image size: {}\".format(size))\n",
    "\n",
    "        # Run the analysis for loop\n",
    "        for data_bin_key in data[size].keys():\n",
    "                \n",
    "            i = 1\n",
    "            total_time = 0\n",
    "            total_steps_count = 0\n",
    "            num_images = len(data[size][data_bin_key])\n",
    "\n",
    "            for start_node, goal_node in pair_node_list:\n",
    "                \n",
    "                # Create start_node and goal_node objects\n",
    "                start_node = Node(start_node)\n",
    "                goal_node = Node(goal_node)\n",
    "\n",
    "                # Iterate over all image in data[size][data_bin_key], for example if we want to access to the list of\n",
    "                # images of size 15 inside the data_bin (0,0.5) -> data[15][(0,0.5)]\n",
    "                for tmp_data in data[size][data_bin_key]:\n",
    "                    \n",
    "                    # Different type of analysis for each finding_neighbors algorithm\n",
    "                    if finding_neighbors ==  'find_first_before_run':\n",
    "                        grid = make_grid(tmp_data, (size, size), 2) # Make grid of neighbors first before run the algorithm\n",
    "                        start_time = time.time() \n",
    "                        steps_count = Astar(grid, tmp_data, start_node, goal_node, (size, size), 2, heuristic)\n",
    "                        running_time = time.time() - start_time # Running time \n",
    "                    else:\n",
    "                        start_time = time.time()\n",
    "                        steps_count = Astar(tmp_data, start_node, goal_node, (size, size), 2, heuristic)\n",
    "                        running_time = time.time() - start_time # Running time             \n",
    "                    \n",
    "                    total_time += running_time\n",
    "                    total_steps_count += steps_count\n",
    "                    \n",
    "            print(\"Standard deviation range: {}\".format(data_bin_key))\n",
    "            \n",
    "            # Average running time\n",
    "            tmp_avg_run_time = total_time / num_images / num_points\n",
    "            analysis_results[-1].avg_run_time[data_bin_key] = tmp_avg_run_time\n",
    "            print(\"Average running time: {} s.\".format(tmp_avg_run_time))\n",
    "\n",
    "            # Average steps count\n",
    "            tmp_avg_steps_count = total_steps_count // num_images // num_points\n",
    "            analysis_results[-1].avg_steps_count[data_bin_key] = tmp_avg_steps_count\n",
    "            print(\"Average steps counts: {} steps.\".format(tmp_avg_steps_count))\n",
    "                                         \n",
    "        print(\"------------------------------------\")\n",
    "                                         \n",
    "    return analysis_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3cU2BU9CqmP"
   },
   "source": [
    "### 4.1.2. Greedy BFS analysis function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsQvrj8WCtmR"
   },
   "outputs": [],
   "source": [
    "def GreedyBFS_analysis(data, parameters):        \n",
    "    #-------------------------------------- 4 different A* algorithms --------------------------------------\n",
    "    \n",
    "    def norm_GreedyBFS_list(img_array, start_node, goal_node, img_shape, theta, heuristic):\n",
    "        steps_count = 0 # Save our number of steps\n",
    "        visited_nodes = set() # Save our visited nodes\n",
    "        frontier = [start_node] # Add the start_node to the frontier\n",
    "\n",
    "        while len(frontier) > 0: # While there is something inside the frontier, continue the algorithm\n",
    "            \n",
    "            # Sort the frontier in reverse order, here Python uses timsort\n",
    "            frontier = sorted(frontier, reverse = True)\n",
    "            \n",
    "            # Pop the last node, which has the lowest value of f\n",
    "            actual_node = frontier.pop()\n",
    "            \n",
    "            # Add 1 to steps_count for each time we enter a node\n",
    "            steps_count += 1 \n",
    "            \n",
    "            # Save the temporary g-score of the actual node\n",
    "            tmp_g_score = actual_node.g\n",
    "            \n",
    "            # Return the steps_count for analysis process\n",
    "            if actual_node == goal_node:\n",
    "                return steps_count\n",
    "            \n",
    "            # Add the position (tuple(x,y)) to visited_node\n",
    "            visited_nodes.add(actual_node.position)\n",
    "            \n",
    "            # Create a list of temporary visited nodes, add these nodes to the frontier after the for loop below \n",
    "            tmp_visited_nodes = []\n",
    "\n",
    "            # Iterate over all neighbors of the actual node\n",
    "            for next_pos in next_pos_list(img_array, actual_node, img_shape, theta):\n",
    "                steps_count += 1 # Add 1 for each time we check the next node\n",
    "                next_node = Node(next_pos) # Create next_node with position next_pos\n",
    "                \n",
    "                # If next_pos in visited_nodes, we don't do anything\n",
    "                if next_pos not in visited_nodes:\n",
    "\n",
    "                    # f = h, update f-score\n",
    "                    next_node.h = heuristic(next_node, start_node, goal_node, delta) \n",
    "                    next_node.f = next_node.h\n",
    "                    next_node.parent = actual_node\n",
    "\n",
    "                    # If next_node in frontier\n",
    "                    if next_node in frontier:\n",
    "                        idx = frontier.index(next_node) # Find the index of next_node in the frontier\n",
    "                        \n",
    "                        # If the f-score of the duplicate node is more than that of the actual next_node\n",
    "                        if frontier[idx] > next_node: \n",
    "                            frontier.remove(next_node) # Remove this next_node in the frontier\n",
    "                            tmp_visited_nodes.append(next_node) # And append the actual next_node to the frontier\n",
    "                    else:\n",
    "                        tmp_visited_nodes.append(next_node) # Append the actual next_node to the frontier\n",
    "            \n",
    "            # Add all the nodes inside tmp_visited_nodes into frontier\n",
    "            frontier += tmp_visited_nodes\n",
    "        \n",
    "        # Return the number of steps counted for analysis process\n",
    "        return steps_count\n",
    "    \n",
    "    \n",
    "    def norm_GreedyBFS_priority_queue(img_array, start_node, goal_node, img_shape, theta, heuristic):\n",
    "        steps_count = 0 #Save our number of steps\n",
    "        visited_nodes = set() # Save our visited nodes\n",
    "        frontier = queue.PriorityQueue()\n",
    "        frontier.put((0, start_node))\n",
    "\n",
    "        while frontier.empty() == False:\n",
    "            actual_node = frontier.get()[1]        \n",
    "            steps_count += 1 # Add 1 for each time we enter a node\n",
    "            \n",
    "            # Save the temporary g-score of the actual node\n",
    "            tmp_g_score = actual_node.g\n",
    "\n",
    "            if actual_node == goal_node:\n",
    "                return steps_count\n",
    "\n",
    "            visited_nodes.add(actual_node.position)\n",
    "\n",
    "            # Iterate over all neighbors of the actual node\n",
    "            for next_pos in next_pos_list(img_array, actual_node, img_shape, theta):\n",
    "                steps_count += 1 # Add 1 for each time we check the next node\n",
    "                next_node = Node(next_pos) # Create next_node with position next_pos\n",
    "                \n",
    "                # If next_pos in visited_nodes, we don't do anything\n",
    "                if next_pos not in visited_nodes:\n",
    "\n",
    "                     # f = h, update f-score\n",
    "                    next_node.h = heuristic(next_node, start_node, goal_node, delta) \n",
    "                    next_node.f = next_node.h\n",
    "                    next_node.parent = actual_node\n",
    "\n",
    "                    # Check if there exists a node having the same position as next_node (denoted as duplicate node) \n",
    "                    # in the frontier or not\n",
    "                    if (next_node.f, next_node) in frontier.queue:\n",
    "                        # Find the index of the duplicate node in the frontier.queue\n",
    "                        idx = frontier.queue.index((next_node.f, next_node)) \n",
    "\n",
    "                        # Check the f-value of the 2 nodes (next_node and the duplicate node in the frontier)\n",
    "                        if frontier.queue[idx][0] > next_node.f:\n",
    "                            \n",
    "                            # Replace the duplicate node by the next_node with lower f-score\n",
    "                            # Step 1: Create a temporary queue\n",
    "                            tmp_queue = queue.PriorityQueue()\n",
    "                            \n",
    "                            # Step 2: Put all the elements of the frontier to tmp_queue until we find the duplicate node\n",
    "                            while t != frontier.queue[idx]:\n",
    "                                t = frontier.get()\n",
    "                                tmp_queue.put(t)\n",
    "                            \n",
    "                            # Step 3: Delete the duplicate node\n",
    "                            t = frontier.get()\n",
    "                            \n",
    "                            # Step 4: Reput all the elements from the tmp_queue to the frontier\n",
    "                            while tmp_queue.empty() == False:\n",
    "                                t = tmp_queue.get()\n",
    "                                frontier.put(t)    \n",
    "                                \n",
    "                            frontier.put((next_node.f, next_node)) # Put next_node to frontier\n",
    "                    else:\n",
    "                        frontier.put((next_node.f, next_node)) # Put next_node to frontier\n",
    "        \n",
    "        # Return the number of steps counted for analysis process\n",
    "        return steps_count\n",
    "    \n",
    "    \n",
    "    def GreedyBFS_list_with_init(grid, img_array, start_node, goal_node, img_shape, theta, heuristic):    \n",
    "        steps_count = 0 #Save our number of steps\n",
    "        visited_nodes = set() # Save our visited nodes\n",
    "        frontier = [start_node] # Add the start_node to the frontier\n",
    "\n",
    "        while len(frontier) > 0: # While there is something inside the frontier, continue the algorithm\n",
    "            \n",
    "            # Sort the frontier in reverse order, here Python uses timsort\n",
    "            frontier = sorted(frontier, reverse = True)\n",
    "            \n",
    "            # Pop the last node, which has the lowest value of f\n",
    "            actual_node = frontier.pop()\n",
    "            \n",
    "            # Add 1 to steps_count for each time we enter a node\n",
    "            steps_count += 1 \n",
    "            \n",
    "            # Save the temporary g-score of the actual node\n",
    "            tmp_g_score = actual_node.g\n",
    "            \n",
    "            # Return the steps_count for analysis process\n",
    "            if actual_node == goal_node:\n",
    "                return steps_count\n",
    "            \n",
    "            # Add the position (tuple(x,y)) to visited_node\n",
    "            visited_nodes.add(actual_node.position)\n",
    "            \n",
    "            # Create a list of temporary visited nodes, add these nodes to the frontier after the for loop below \n",
    "            tmp_visited_nodes = []\n",
    "\n",
    "            # Iterate over all neighbors of the actual node\n",
    "            for next_pos in grid[actual_node.position[0]][actual_node.position[1]]:\n",
    "                steps_count += 1 # Add 1 for each time we check the next node\n",
    "                next_node = Node(next_pos) # Create next_node with position next_pos\n",
    "                \n",
    "                # If next_pos in visited_nodes, we don't do anything\n",
    "                if next_pos not in visited_nodes:\n",
    "\n",
    "                    # f = h, update f-score\n",
    "                    next_node.h = heuristic(next_node, start_node, goal_node, delta) \n",
    "                    next_node.f = next_node.h\n",
    "                    next_node.parent = actual_node\n",
    "\n",
    "                    # If next_node in frontier\n",
    "                    if next_node in frontier:\n",
    "                        idx = frontier.index(next_node) # Find the index of next_node in the frontier\n",
    "                        \n",
    "                        # If the f-score of the duplicate node is more than that of the actual next_node\n",
    "                        if frontier[idx] > next_node: \n",
    "                            frontier.remove(next_node) # Remove this next_node in the frontier\n",
    "                            tmp_visited_nodes.append(next_node) # And append the actual next_node to the frontier\n",
    "                    else:\n",
    "                        tmp_visited_nodes.append(next_node) # Append the actual next_node to the frontier\n",
    "            \n",
    "            # Add all the nodes inside tmp_visited_nodes into frontier\n",
    "            frontier += tmp_visited_nodes\n",
    "        \n",
    "        # Return the number of steps counted for analysis process\n",
    "        return steps_count\n",
    "    \n",
    "    \n",
    "    #------------------------------------ Retrieving data from parameters ------------------------------------\n",
    "    \n",
    "    #List of image size:\n",
    "    img_size = parameters['img_size']\n",
    "    \n",
    "    #Type of frontier (list or priority_queue)\n",
    "    frontier_type = parameters['frontier_type']    \n",
    "    \n",
    "    #Type of finding neighbors (find_first_before_run or find_while_running   finding_neighbors = parameters['finding_neighbors']\n",
    "    finding_neighbors = parameters['finding_neighbors']\n",
    "    \n",
    "    #Choose GreedyBFS version\n",
    "    if finding_neighbors == 'find_while_running' and frontier_type == 'list':\n",
    "        GreedyBFS = norm_GreedyBFS_list \n",
    "    elif finding_neighbors == 'find_while_running' and frontier_type == 'priority_queue':\n",
    "        GreedyBFS = norm_GreedyBFS_priority_queue\n",
    "    elif finding_neighbors == 'find_first_before_run' and frontier_type == 'list':\n",
    "        GreedyBFS = GreedyBFS_list_with_init\n",
    "    \n",
    "    #Type of heuristic function (manhattan; tie_breaking_high_g_cost)\n",
    "    global heuristics\n",
    "    heuristic_type = parameters['heuristic_type'] \n",
    "    heuristic = heuristics[heuristic_type]\n",
    "    \n",
    "    #Delta for heuristic functions\n",
    "    delta = 0.001\n",
    "    if 'delta' in parameters.keys():\n",
    "        delta = parameters['delta']\n",
    "\n",
    "    #Number of random starting points and ending points to run analysis\n",
    "    num_points = parameters['num_points'] \n",
    "    \n",
    "    #-------------------------------------------- Analysis process --------------------------------------------\n",
    "    # Store the analysis results\n",
    "    analysis_results = []\n",
    "    \n",
    "    # Iterate over all the image size in data.keys()\n",
    "    for size in data.keys():\n",
    "        \n",
    "        # Preparation\n",
    "        analysis_results.append(AnalysisResults(size)) # Create a new AnalysisResults object and append it to the list\n",
    "        pair_node_list = random_initialization(num_points, size) # Create a random pair of start node and goal node\n",
    "        print(\"Image size: {}\".format(size))\n",
    "\n",
    "        # Run the analysis for loop\n",
    "        for data_bin_key in data[size].keys():\n",
    "                \n",
    "            i = 1\n",
    "            total_time = 0\n",
    "            total_steps_count = 0\n",
    "            num_images = len(data[size][data_bin_key])\n",
    "\n",
    "            for start_node, goal_node in pair_node_list:\n",
    "                \n",
    "                # Create start_node and goal_node objects\n",
    "                start_node = Node(start_node)\n",
    "                goal_node = Node(goal_node)\n",
    "\n",
    "                # Iterate over all image in data[size][data_bin_key], for example if we want to access to the list of\n",
    "                # images of size 15 inside the data_bin (0,0.5) -> data[15][(0,0.5)]\n",
    "                for tmp_data in data[size][data_bin_key]:\n",
    "                    \n",
    "                    # Different type of analysis for each finding_neighbors algorithm\n",
    "                    if finding_neighbors ==  'find_first_before_run': \n",
    "                        grid = make_grid(tmp_data, (size, size), 2) # Make grid of neighbors first before run the algorithm\n",
    "                        start_time = time.time()\n",
    "                        steps_count = GreedyBFS(grid, tmp_data, start_node, goal_node, (size, size), 2, heuristic)\n",
    "                        running_time = time.time() - start_time # Running time\n",
    "                    else:\n",
    "                        start_time = time.time()\n",
    "                        steps_count = GreedyBFS(tmp_data, start_node, goal_node, (size, size), 2, heuristic)\n",
    "                        running_time = time.time() - start_time # Running time  \n",
    "                    \n",
    "                    total_time += running_time\n",
    "                    total_steps_count += steps_count\n",
    "\n",
    "            print(\"Standard deviation range: {}\".format(data_bin_key))\n",
    "            \n",
    "            # Average running time\n",
    "            tmp_avg_run_time = total_time / num_images / num_points\n",
    "            analysis_results[-1].avg_run_time[data_bin_key] = tmp_avg_run_time\n",
    "            print(\"Average running time: {} s.\".format(tmp_avg_run_time))\n",
    "\n",
    "            # Average steps count\n",
    "            tmp_avg_steps_count = total_steps_count // num_images // num_points\n",
    "            analysis_results[-1].avg_steps_count[data_bin_key] = tmp_avg_steps_count\n",
    "            print(\"Average steps counts: {} steps.\".format(tmp_avg_steps_count))\n",
    "                                         \n",
    "        print(\"------------------------------------\")\n",
    "                                         \n",
    "    return analysis_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBLVOQlDC1Xm"
   },
   "source": [
    "### 4.1.3. Uniform cost search analysis function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWlLLs-jC9Cn"
   },
   "outputs": [],
   "source": [
    "def UCS_analysis(data, parameters):        \n",
    "    #-------------------------------------- 4 different A* algorithms --------------------------------------\n",
    "    \n",
    "    def norm_UCS_list(img_array, start_node, goal_node, img_shape, theta, heuristic):\n",
    "        steps_count = 0 #Save our number of steps\n",
    "        visited_nodes = set() # Save our visited nodes\n",
    "        frontier = [start_node] # Add the start_node to the frontier\n",
    "\n",
    "        while len(frontier) > 0: # While there is something inside the frontier, continue the algorithm\n",
    "            \n",
    "            # Sort the frontier in reverse order, here Python uses timsort\n",
    "            frontier = sorted(frontier, reverse = True)\n",
    "            \n",
    "            # Pop the last node, which has the lowest value of f\n",
    "            actual_node = frontier.pop()\n",
    "            \n",
    "            # Add 1 to steps_count for each time we enter a node\n",
    "            steps_count += 1 \n",
    "            \n",
    "            # Save the temporary g-score of the actual node\n",
    "            tmp_g_score = actual_node.g\n",
    "            \n",
    "            # Return the steps_count for analysis process\n",
    "            if actual_node == goal_node:\n",
    "                return steps_count\n",
    "            \n",
    "            # Add the position (tuple(x,y)) to visited_node\n",
    "            visited_nodes.add(actual_node.position)\n",
    "            \n",
    "            # Create a list of temporary visited nodes, add these nodes to the frontier after the for loop below \n",
    "            tmp_visited_nodes = []\n",
    "\n",
    "            # Iterate over all neighbors of the actual node\n",
    "            for next_pos in next_pos_list(img_array, actual_node, img_shape, theta):\n",
    "                steps_count += 1 # Add 1 for each time we check the next node\n",
    "                next_node = Node(next_pos) # Create next_node with position next_pos\n",
    "                \n",
    "                # If next_pos in visited_nodes, we don't do anything\n",
    "                if next_pos not in visited_nodes:\n",
    "\n",
    "                    # f = g, update f-score\n",
    "                    next_node.g = tmp_g_score + 1\n",
    "                    next_node.f = next_node.g\n",
    "                    next_node.parent = actual_node\n",
    "\n",
    "                    # If next_node in frontier\n",
    "                    if next_node in frontier:\n",
    "                        idx = frontier.index(next_node) # Find the index of next_node in the frontier\n",
    "                        \n",
    "                        # If the f-score of the duplicate node is more than that of the actual next_node\n",
    "                        if frontier[idx] > next_node: \n",
    "                            frontier.remove(next_node) # Remove this next_node in the frontier\n",
    "                            tmp_visited_nodes.append(next_node) # And append the actual next_node to the frontier\n",
    "                    else:\n",
    "                        tmp_visited_nodes.append(next_node) # Append the actual next_node to the frontier\n",
    "            \n",
    "            # Add all the nodes inside tmp_visited_nodes into frontier\n",
    "            frontier += tmp_visited_nodes\n",
    "        \n",
    "        # Return the number of steps counted for analysis process\n",
    "        return steps_count\n",
    "    \n",
    "\n",
    "    def norm_UCS_priority_queue(img_array, start_node, goal_node, img_shape, theta, heuristic):\n",
    "        steps_count = 0 #Save our number of steps\n",
    "        visited_nodes = set() # Save our visited nodes\n",
    "        frontier = queue.PriorityQueue()\n",
    "        frontier.put((0, start_node))\n",
    "\n",
    "        while frontier.empty() == False:\n",
    "            actual_node = frontier.get()[1]        \n",
    "            steps_count += 1 # Add 1 for each time we enter a node\n",
    "            tmp_g_score = actual_node.g\n",
    "\n",
    "            if actual_node == goal_node:\n",
    "                return steps_count\n",
    "\n",
    "            visited_nodes.add(actual_node.position)\n",
    "\n",
    "            # Iterate over all neighbors of the actual node\n",
    "            for next_pos in next_pos_list(img_array, actual_node, img_shape, theta):\n",
    "                steps_count += 1 # Add 1 for each time we check the next node\n",
    "                next_node = Node(next_pos) # Create next_node with position next_pos\n",
    "                \n",
    "                # If next_pos in visited_nodes, we don't do anything\n",
    "                if next_pos not in visited_nodes:\n",
    "\n",
    "                    # f = g, update f-score\n",
    "                    next_node.g = tmp_g_score + 1\n",
    "                    next_node.f = next_node.g\n",
    "                    next_node.parent = actual_node\n",
    "\n",
    "                    # Check if there exists a node having the same position as next_node (denoted as duplicate node) \n",
    "                    # in the frontier or not\n",
    "                    if (next_node.f, next_node) in frontier.queue:\n",
    "                        # Find the index of the duplicate node in the frontier.queue\n",
    "                        idx = frontier.queue.index((next_node.f, next_node)) \n",
    "\n",
    "                        # Check the f-value of the 2 nodes (next_node and the duplicate node in the frontier)\n",
    "                        if frontier.queue[idx][0] > next_node.f:\n",
    "                            \n",
    "                            # Replace the duplicate node by the next_node with lower f-score\n",
    "                            # Step 1: Create a temporary queue\n",
    "                            tmp_queue = queue.PriorityQueue()\n",
    "                            \n",
    "                            # Step 2: Put all the elements of the frontier to tmp_queue until we find the duplicate node\n",
    "                            while t != frontier.queue[idx]:\n",
    "                                t = frontier.get()\n",
    "                                tmp_queue.put(t)\n",
    "                            \n",
    "                            # Step 3: Delete the duplicate node\n",
    "                            t = frontier.get()\n",
    "                            \n",
    "                            # Step 4: Reput all the elements from the tmp_queue to the frontier\n",
    "                            while tmp_queue.empty() == False:\n",
    "                                t = tmp_queue.get()\n",
    "                                frontier.put(t)    \n",
    "                                \n",
    "                            frontier.put((next_node.f, next_node)) # Put next_node to frontier\n",
    "                    else:\n",
    "                        frontier.put((next_node.f, next_node)) # Put next_node to frontier\n",
    "        \n",
    "        # Return the number of steps counted for analysis process\n",
    "        return steps_count\n",
    "    \n",
    "    \n",
    "    def UCS_list_with_init(grid, img_array, start_node, goal_node, img_shape, theta, heuristic):    \n",
    "        steps_count = 0 #Save our number of steps\n",
    "        visited_nodes = set() # Save our visited nodes\n",
    "        frontier = [start_node] # Add the start_node to the frontier\n",
    "\n",
    "        while len(frontier) > 0: # While there is something inside the frontier, continue the algorithm\n",
    "            \n",
    "            # Sort the frontier in reverse order, here Python uses timsort\n",
    "            frontier = sorted(frontier, reverse = True)\n",
    "            \n",
    "            # Pop the last node, which has the lowest value of f\n",
    "            actual_node = frontier.pop()\n",
    "            \n",
    "            # Add 1 to steps_count for each time we enter a node\n",
    "            steps_count += 1 \n",
    "            \n",
    "            # Save the temporary g-score of the actual node\n",
    "            tmp_g_score = actual_node.g\n",
    "            \n",
    "            # Return the steps_count for analysis process\n",
    "            if actual_node == goal_node:\n",
    "                return steps_count\n",
    "            \n",
    "            # Add the position (tuple(x,y)) to visited_node\n",
    "            visited_nodes.add(actual_node.position)\n",
    "            \n",
    "            # Create a list of temporary visited nodes, add these nodes to the frontier after the for loop below \n",
    "            tmp_visited_nodes = []\n",
    "\n",
    "            for next_pos in grid[actual_node.position[0]][actual_node.position[1]]:\n",
    "                steps_count += 1 #Add 1 for each time we check the next node\n",
    "                next_node = Node(next_pos)\n",
    "\n",
    "                if next_pos not in visited_nodes:\n",
    "\n",
    "                    # f = g, update f-score\n",
    "                    next_node.g = tmp_g_score + 1\n",
    "                    next_node.f = next_node.g\n",
    "                    next_node.parent = actual_node\n",
    "\n",
    "                    # If next_node in frontier\n",
    "                    if next_node in frontier:\n",
    "                        idx = frontier.index(next_node) # Find the index of next_node in the frontier\n",
    "                        \n",
    "                        # If the f-score of the duplicate node is more than that of the actual next_node\n",
    "                        if frontier[idx] > next_node: \n",
    "                            frontier.remove(next_node) # Remove this next_node in the frontier\n",
    "                            tmp_visited_nodes.append(next_node) # And append the actual next_node to the frontier\n",
    "                    else:\n",
    "                        tmp_visited_nodes.append(next_node) # Append the actual next_node to the frontier\n",
    "            \n",
    "            # Add all the nodes inside tmp_visited_nodes into frontier\n",
    "            frontier += tmp_visited_nodes\n",
    "        \n",
    "        # Return the number of steps counted for analysis process\n",
    "        return steps_count\n",
    "    \n",
    "    \n",
    "    #------------------------------------ Retrieving data from parameters ------------------------------------\n",
    "    \n",
    "    #List of image size:\n",
    "    img_size = parameters['img_size']\n",
    "    \n",
    "    #Type of frontier (list or priority_queue)\n",
    "    frontier_type = parameters['frontier_type']    \n",
    "    \n",
    "    #Type of finding neighbors (find_first_before_run or find_while_running)\n",
    "    finding_neighbors = parameters['finding_neighbors']\n",
    "    \n",
    "    #Choose UCS version\n",
    "    if finding_neighbors == 'find_while_running' and frontier_type == 'list':\n",
    "        UCS = norm_UCS_list \n",
    "    elif finding_neighbors == 'find_while_running' and frontier_type == 'priority_queue':\n",
    "        UCS = norm_UCS_priority_queue\n",
    "    elif finding_neighbors == 'find_first_before_run' and frontier_type == 'list':\n",
    "        UCS = UCS_list_with_init\n",
    "    \n",
    "    #Type of heuristic function (manhattan; tie_breaking_high_g_cost)\n",
    "    global heuristics\n",
    "    heuristic_type = parameters['heuristic_type'] \n",
    "    heuristic = heuristics[heuristic_type]\n",
    "    \n",
    "    #Delta for heuristic functions\n",
    "    delta = 0.001\n",
    "    if 'delta' in parameters.keys():\n",
    "        delta = parameters['delta']\n",
    "\n",
    "    #Number of random starting points and ending points to run analysis\n",
    "    num_points = parameters['num_points'] \n",
    "    \n",
    "    \n",
    "    #-------------------------------------------- Analysis process --------------------------------------------\n",
    "    # Store the analysis results\n",
    "    analysis_results = []\n",
    "    \n",
    "    # Iterate over all the image size in data.keys()\n",
    "    for size in data.keys():\n",
    "        \n",
    "        # Preparation\n",
    "        analysis_results.append(AnalysisResults(size)) # Create a new AnalysisResults object and append it to the list\n",
    "        pair_node_list = random_initialization(num_points, size) # Create a random pair of start node and goal node\n",
    "        print(\"Image size: {}\".format(size))\n",
    "\n",
    "        # Run the analysis for loop\n",
    "        for data_bin_key in data[size].keys():\n",
    "                \n",
    "            i = 1\n",
    "            total_time = 0\n",
    "            total_steps_count = 0\n",
    "            num_images = len(data[size][data_bin_key])\n",
    "\n",
    "            for start_node, goal_node in pair_node_list:\n",
    "                \n",
    "                # Create start_node and goal_node objects\n",
    "                start_node = Node(start_node)\n",
    "                goal_node = Node(goal_node)\n",
    "                \n",
    "                # Iterate over all image in data[size][data_bin_key], for example if we want to access to the list of\n",
    "                # images of size 15 inside the data_bin (0,0.5) -> data[15][(0,0.5)]\n",
    "                for tmp_data in data[size][data_bin_key]:\n",
    "                    \n",
    "                    # Different type of analysis for each finding_neighbors algorithm\n",
    "                    if finding_neighbors ==  'find_first_before_run':\n",
    "                        grid = make_grid(tmp_data, (size, size), 2) # Make grid of neighbors first before run the algorithm\n",
    "                        start_time = time.time()\n",
    "                        steps_count = UCS(grid, tmp_data, start_node, goal_node, (size, size), 2, heuristic)\n",
    "                        running_time = time.time() - start_time # Running time\n",
    "                    else:\n",
    "                        start_time = time.time()\n",
    "                        steps_count = UCS(tmp_data, start_node, goal_node, (size, size), 2, heuristic)\n",
    "                        running_time = time.time() - start_time # Running time          \n",
    "                    \n",
    "                    total_time += running_time\n",
    "                    total_steps_count += steps_count\n",
    "\n",
    "            print(\"Standard deviation range: {}\".format(data_bin_key))\n",
    "            \n",
    "            # Average running time\n",
    "            tmp_avg_run_time = total_time / num_images / num_points\n",
    "            analysis_results[-1].avg_run_time[data_bin_key] = tmp_avg_run_time\n",
    "            print(\"Average running time: {} s.\".format(tmp_avg_run_time))\n",
    "\n",
    "            # Average steps count\n",
    "            tmp_avg_steps_count = total_steps_count // num_images // num_points\n",
    "            analysis_results[-1].avg_steps_count[data_bin_key] = tmp_avg_steps_count\n",
    "            print(\"Average steps counts: {} steps.\".format(tmp_avg_steps_count))\n",
    "                                         \n",
    "        print(\"------------------------------------\")\n",
    "                                         \n",
    "    return analysis_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e11pKFuvC-rH"
   },
   "source": [
    "## 4.2. Analysis Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzHLHsVlDH1f"
   },
   "source": [
    "### 4.2.1. Comparison of frontier definitions\n",
    "#### 4.2.1.1. Astar\n",
    "**List (with timsort)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pso8VcxsDFBC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'img_size': img_size,\n",
    "    'frontier_type': 'list',\n",
    "    'finding_neighbors': 'find_while_running',\n",
    "    'heuristic_type': 'manhattan', \n",
    "    'num_points': 100\n",
    "}\n",
    "\n",
    "analysis_results = Astar_analysis(data, parameters)\n",
    "\n",
    "# Save the data\n",
    "tmp_path = path + '/comparison_of_frontier_definitions'\n",
    "\n",
    "if os.path.exists(tmp_path) == False:\n",
    "    os.mkdir(tmp_path)\n",
    "    \n",
    "tmp_path = tmp_path + '/' + 'Astar - ' + parameters['frontier_type'] + ' - result.pkl'\n",
    "\n",
    "with open(tmp_path, 'wb') as f:\n",
    "    pickle.dump(analysis_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcTkWwSwD0Lm"
   },
   "source": [
    "**Priority queue (heapsort)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXOs9W4mD2xF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'img_size': img_size,\n",
    "    'frontier_type': 'priority_queue',\n",
    "    'finding_neighbors': 'find_while_running',\n",
    "    'heuristic_type': 'manhattan', \n",
    "    'num_points': 100\n",
    "}\n",
    "\n",
    "analysis_results = Astar_analysis(data, parameters)\n",
    "\n",
    "# Save the data\n",
    "tmp_path = path + '/comparison_of_frontier_definitions'\n",
    "\n",
    "if os.path.exists(tmp_path) == False:\n",
    "    os.mkdir(tmp_path)\n",
    "    \n",
    "tmp_path = tmp_path + '/' + 'Astar - ' + parameters['frontier_type'] + ' - result.pkl'\n",
    "\n",
    "with open(tmp_path, 'wb') as f:\n",
    "    pickle.dump(analysis_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5u7n4p4SIXrN"
   },
   "source": [
    "#### 4.2.1.2. GreedyBFS\n",
    "**List (with timsort)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Z8DqkpHIb4U",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'img_size': img_size,\n",
    "    'frontier_type': 'list',\n",
    "    'finding_neighbors': 'find_while_running',\n",
    "    'heuristic_type': 'manhattan', \n",
    "    'num_points': 100\n",
    "}\n",
    "\n",
    "analysis_results = GreedyBFS_analysis(data, parameters)\n",
    "\n",
    "# Save the data\n",
    "tmp_path = path + '/comparison_of_frontier_definitions'\n",
    "\n",
    "if os.path.exists(tmp_path) == False:\n",
    "    os.mkdir(tmp_path)\n",
    "    \n",
    "tmp_path = tmp_path + '/' + 'GreedyBFS - ' + parameters['frontier_type'] + ' - result.pkl'\n",
    "\n",
    "with open(tmp_path, 'wb') as f:\n",
    "    pickle.dump(analysis_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6eM-8YJIfhd"
   },
   "source": [
    "**Priority queue (heapsort)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4Emf5h4IenF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'img_size': img_size,\n",
    "    'frontier_type': 'priority_queue',\n",
    "    'finding_neighbors': 'find_while_running',\n",
    "    'heuristic_type': 'manhattan', \n",
    "    'num_points': 100\n",
    "}\n",
    "\n",
    "analysis_results = GreedyBFS_analysis(data, parameters)\n",
    "\n",
    "# Save the data\n",
    "tmp_path = path + '/comparison_of_frontier_definitions'\n",
    "\n",
    "if os.path.exists(tmp_path) == False:\n",
    "    os.mkdir(tmp_path)\n",
    "    \n",
    "tmp_path = tmp_path + '/' + 'GreedyBFS - ' + parameters['frontier_type'] + ' - result.pkl'\n",
    "\n",
    "with open(tmp_path, 'wb') as f:\n",
    "    pickle.dump(analysis_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6XkgcmnJM4l"
   },
   "source": [
    "#### 4.2.1.3. UCS\n",
    "**List (with timsort)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "siamcLZGJPJ8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'img_size': img_size,\n",
    "    'frontier_type': 'list',\n",
    "    'finding_neighbors': 'find_while_running',\n",
    "    'heuristic_type': 'manhattan', \n",
    "    'num_points': 100\n",
    "}\n",
    "\n",
    "analysis_results = UCS_analysis(data, parameters)\n",
    "\n",
    "# Save the data\n",
    "tmp_path = path + '/comparison_of_frontier_definitions'\n",
    "\n",
    "if os.path.exists(tmp_path) == False:\n",
    "    os.mkdir(tmp_path)\n",
    "    \n",
    "tmp_path = tmp_path + '/' + 'UCS - ' + parameters['frontier_type'] + ' - result.pkl'\n",
    "\n",
    "with open(tmp_path, 'wb') as f:\n",
    "    pickle.dump(analysis_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "To0I0OomJS51"
   },
   "source": [
    "**Priority queue (heapsort)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJWRd7UyJUHc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'img_size': img_size,\n",
    "    'frontier_type': 'priority_queue',\n",
    "    'finding_neighbors': 'find_while_running',\n",
    "    'heuristic_type': 'manhattan', \n",
    "    'num_points': 100\n",
    "}\n",
    "\n",
    "analysis_results = UCS_analysis(data, parameters)\n",
    "\n",
    "# Save the data\n",
    "tmp_path = path + '/comparison_of_frontier_definitions'\n",
    "\n",
    "if os.path.exists(tmp_path) == False:\n",
    "    os.mkdir(tmp_path)\n",
    "    \n",
    "tmp_path = tmp_path + '/' + 'UCS - ' + parameters['frontier_type'] + ' - result.pkl'\n",
    "\n",
    "with open(tmp_path, 'wb') as f:\n",
    "    pickle.dump(analysis_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3WQISSSEX4-"
   },
   "source": [
    "### 4.2.2. Comparison of different algorithms and Influence of standard deviation to the running time\n",
    "#### 4.2.2.1. Astar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ciSdpWrdHb-Y",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'img_size': img_size,\n",
    "    'frontier_type': 'list',\n",
    "    'finding_neighbors': 'find_while_running',\n",
    "    'heuristic_type': 'manhattan', \n",
    "    'num_points': 100\n",
    "}\n",
    "\n",
    "analysis_results = Astar_analysis(data, parameters)\n",
    "\n",
    "# Save the data\n",
    "tmp_path = path + '/comparison_of_different_algorithms'\n",
    "\n",
    "if os.path.exists(tmp_path) == False:\n",
    "    os.mkdir(tmp_path)\n",
    "    \n",
    "tmp_path = tmp_path + '/' + 'Astar - ' + parameters['frontier_type'] + ' - result.pkl'\n",
    "\n",
    "with open(tmp_path, 'wb') as f:\n",
    "    pickle.dump(analysis_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVWXyDLKKMsd"
   },
   "source": [
    "#### 4.2.2.2. GreedyBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KwwvR3m8KSak",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'img_size': img_size,\n",
    "    'frontier_type': 'list',\n",
    "    'finding_neighbors': 'find_while_running',\n",
    "    'heuristic_type': 'manhattan', \n",
    "    'num_points': 100\n",
    "}\n",
    "\n",
    "analysis_results = GreedyBFS_analysis(data, parameters)\n",
    "\n",
    "# Save the data\n",
    "tmp_path = path + '/comparison_of_different_algorithms'\n",
    "\n",
    "if os.path.exists(tmp_path) == False:\n",
    "    os.mkdir(tmp_path)\n",
    "    \n",
    "tmp_path = tmp_path + '/' + 'GreedyBFS - ' + parameters['frontier_type'] + ' - result.pkl'\n",
    "\n",
    "with open(tmp_path, 'wb') as f:\n",
    "    pickle.dump(analysis_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRTtu1d1KPcG"
   },
   "source": [
    "#### 4.2.2.3. UCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jL4u6wwuKOU_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'img_size': img_size,\n",
    "    'frontier_type': 'list',\n",
    "    'finding_neighbors': 'find_while_running',\n",
    "    'heuristic_type': 'manhattan', \n",
    "    'num_points': 100\n",
    "}\n",
    "\n",
    "analysis_results = UCS_analysis(data, parameters)\n",
    "\n",
    "# Save the data\n",
    "tmp_path = path + '/comparison_of_different_algorithms'\n",
    "\n",
    "if os.path.exists(tmp_path) == False:\n",
    "    os.mkdir(tmp_path)\n",
    "    \n",
    "tmp_path = tmp_path + '/' + 'UCS - ' + parameters['frontier_type'] + ' - result.pkl'\n",
    "\n",
    "with open(tmp_path, 'wb') as f:\n",
    "    pickle.dump(analysis_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKsmwJwFKhR9"
   },
   "source": [
    "### 4.2.3. Heuristic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = [img_size[2], img_size[4]] # Only image size 25 and 50 for the original img_size = [15,20,25,30,50,75,100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3.1. Tie-breaking High g-cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ex3qZxVMK89O",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'img_size': img_size,\n",
    "    'frontier_type': 'list',\n",
    "    'finding_neighbors': 'find_while_running',\n",
    "    'heuristic_type': 'tie_breaking_high_g_cost', \n",
    "    'num_points': 100,\n",
    "    'delta': 0.01 # Change delta here\n",
    "}\n",
    "\n",
    "analysis_results = Astar_analysis(data, parameters) # Use only Astar\n",
    "\n",
    "# Save the data\n",
    "tmp_path = path + '/heuristic_functions'\n",
    "\n",
    "if os.path.exists(tmp_path) == False:\n",
    "    os.mkdir(tmp_path)\n",
    "    \n",
    "tmp_path = tmp_path + '/' + 'tie-breaking-high-g-cost - ' + ' delta = ' + str(parameters['delta']) + ' - result.pkl'\n",
    "\n",
    "with open(tmp_path, 'wb') as f:\n",
    "    pickle.dump(analysis_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqsbtILJLNgt"
   },
   "source": [
    "#### 4.2.3.2. Variance of Tie-breaking High g-cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZ3R_PnkLM_8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'img_size': img_size,\n",
    "    'frontier_type': 'list',\n",
    "    'finding_neighbors': 'find_while_running',\n",
    "    'heuristic_type': 'var_tie_breaking_high_g_cost', \n",
    "    'num_points': 100,\n",
    "    'delta': 0.01 # Change delta here\n",
    "}\n",
    "\n",
    "analysis_results = Astar_analysis(data, parameters) # Use only Astar\n",
    "\n",
    "# Save the data\n",
    "tmp_path = path + '/heuristic_functions'\n",
    "\n",
    "if os.path.exists(tmp_path) == False:\n",
    "    os.mkdir(tmp_path)\n",
    "    \n",
    "tmp_path = tmp_path + '/' + 'variance-of-tie-breaking-high-g-cost - ' + ' delta = ' + str(parameters['delta']) + ' - result.pkl'\n",
    "\n",
    "with open(tmp_path, 'wb') as f:\n",
    "    pickle.dump(analysis_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28iqohGOLSEt"
   },
   "source": [
    "#### 4.2.3.3. Tie-breaking Low g-cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-j6SiUwfLUVj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'img_size': img_size,\n",
    "    'frontier_type': 'list',\n",
    "    'finding_neighbors': 'find_while_running',\n",
    "    'heuristic_type': 'tie_breaking_low_g_cost', \n",
    "    'num_points': 100,\n",
    "    'delta': 0.01 # Change delta here\n",
    "}\n",
    "\n",
    "analysis_results = Astar_analysis(data, parameters) # Use only Astar\n",
    "\n",
    "# Save the data\n",
    "tmp_path = path + '/heuristic_functions'\n",
    "\n",
    "if os.path.exists(tmp_path) == False:\n",
    "    os.mkdir(tmp_path)\n",
    "    \n",
    "tmp_path = tmp_path + '/' + 'tie-breaking-low-g-cost - ' + ' delta = ' + str(parameters['delta']) + ' - result.pkl'\n",
    "\n",
    "with open(tmp_path, 'wb') as f:\n",
    "    pickle.dump(analysis_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KTN3rCgLUuV"
   },
   "source": [
    "#### 4.2.3.4. Variance of Tie-breaking Low g-cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2A-I2ii0LWZe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'img_size': img_size,\n",
    "    'frontier_type': 'list',\n",
    "    'finding_neighbors': 'find_while_running',\n",
    "    'heuristic_type': 'var_tie_breaking_low_g_cost', \n",
    "    'num_points': 100,\n",
    "    'delta': 0.01 # Change delta here\n",
    "}\n",
    "\n",
    "analysis_results = Astar_analysis(data, parameters) # Use only Astar\n",
    "\n",
    "# Save the data\n",
    "tmp_path = path + '/heuristic_functions'\n",
    "\n",
    "if os.path.exists(tmp_path) == False:\n",
    "    os.mkdir(tmp_path)\n",
    "    \n",
    "tmp_path = tmp_path + '/' + 'variance-of-tie-breaking-low-g-cost - ' + ' delta = ' + str(parameters['delta']) + ' - result.pkl'\n",
    "\n",
    "with open(tmp_path, 'wb') as f:\n",
    "    pickle.dump(analysis_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOdU0qClNPDG"
   },
   "source": [
    "### 4.2.4. Find neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgq8wSgvN3xV"
   },
   "source": [
    "**Find while running**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wt9wir7-NSTF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'img_size': img_size,\n",
    "    'frontier_type': 'list',\n",
    "    'finding_neighbors': 'find_while_running',\n",
    "    'heuristic_type': 'manhattan', \n",
    "    'num_points': 100,\n",
    "}\n",
    "\n",
    "analysis_results = Astar_analysis(data, parameters) # Use only Astar\n",
    "\n",
    "# Save the data\n",
    "tmp_path = path + '/find_neighbors'\n",
    "\n",
    "if os.path.exists(tmp_path) == False:\n",
    "    os.mkdir(tmp_path)\n",
    "    \n",
    "tmp_path = tmp_path + '/' + 'find_while_running - result.pkl'\n",
    "\n",
    "with open(tmp_path, 'wb') as f:\n",
    "    pickle.dump(analysis_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoOxSQFLNy_E"
   },
   "source": [
    "**Find first before run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMnccTHXN2T-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'img_size': img_size,\n",
    "    'frontier_type': 'list',\n",
    "    'finding_neighbors': 'find_first_before_run',\n",
    "    'heuristic_type': 'manhattan', \n",
    "    'num_points': 100\n",
    "}\n",
    "\n",
    "analysis_results = Astar_analysis(data, parameters) # Use only Astar\n",
    "\n",
    "# Save the data\n",
    "tmp_path = path + '/find_neighbors'\n",
    "\n",
    "if os.path.exists(tmp_path) == False:\n",
    "    os.mkdir(tmp_path)\n",
    "    \n",
    "tmp_path = tmp_path + '/' + 'find_first_before_run - result.pkl'\n",
    "\n",
    "with open(tmp_path, 'wb') as f:\n",
    "    pickle.dump(analysis_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Path visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image  # work with images\n",
    "import numpy as np\n",
    "import queue\n",
    "import os\n",
    "import pygame\n",
    "import time\n",
    "\n",
    "file_name = \"Mars_MGS_MOLA_DEM.jpg\"  # The name of the file we want to save\n",
    "img = Image.open(file_name)  # Open our saved file above\n",
    "img = img.convert(\"L\")  # Convert to one channel grey image\n",
    "img_array = np.asarray(img)  # Convert the image to an array\n",
    "img_array = img_array.astype('int32')\n",
    "img = img_array[3000:3050, 3000:3050].astype('float64')\n",
    "\n",
    "WHITE = (255, 255, 255)\n",
    "PURPLE = (128, 0, 128)\n",
    "GREY = (128, 128, 128)\n",
    "TURQUOISE = (64, 224, 208)\n",
    "RED = (255, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "YELLOW = (255, 255, 0)\n",
    "\n",
    "\n",
    "class Node:\n",
    "    length = len(img)\n",
    "\n",
    "    def __init__(self, row, col, width):\n",
    "        self.row = row\n",
    "        self.col = col\n",
    "        self.x = row * width\n",
    "        self.y = col * width\n",
    "        self.color = WHITE\n",
    "        self.neighbors = []\n",
    "        self.width = width\n",
    "        self.height = img[col][row]\n",
    "        self.g = float('inf')\n",
    "        self.f = float('inf')\n",
    "        self.parent = None\n",
    "\n",
    "    def get_position(self):\n",
    "        return self.row, self.col\n",
    "\n",
    "    def is_neighbor(self, other, gradient):\n",
    "        return abs(self.height - other.height) <= gradient\n",
    "\n",
    "    def get_neighbors(self, grid, gradient):\n",
    "        if self.row < Node.length - 1 and self.is_neighbor(grid[self.row + 1][self.col], gradient):\n",
    "            self.neighbors.append(grid[self.row + 1][self.col])\n",
    "\n",
    "        if self.row > 0 and self.is_neighbor(grid[self.row - 1][self.col], gradient):\n",
    "            self.neighbors.append(grid[self.row - 1][self.col])\n",
    "\n",
    "        if self.col < Node.length - 1 and self.is_neighbor(grid[self.row][self.col + 1], gradient):\n",
    "            self.neighbors.append(grid[self.row][self.col + 1])\n",
    "\n",
    "        if self.col > 0 and self.is_neighbor(grid[self.row][self.col - 1], gradient):\n",
    "            self.neighbors.append(grid[self.row][self.col - 1])\n",
    "\n",
    "    def draw(self, screen):\n",
    "        pygame.draw.rect(screen, self.color,\n",
    "                         (self.x, self.y, self.width, self.width))\n",
    "\n",
    "\n",
    "# Manhattan distance\n",
    "def manhattan(node, goal):\n",
    "    x_node, y_node = node.get_position()\n",
    "    x_goal, y_goal = goal.get_position()\n",
    "    return abs(x_node - x_goal) + abs(y_node - y_goal)\n",
    "\n",
    "# There are many states with the same f-cost, and we have to choose the order in which to expand them.\n",
    "# For tie_breaking_high_g_cost, we preferred states closer to the goal node than the goal state.\n",
    "\n",
    "\n",
    "def tie_breaking_high_g_cost(node, goal, delta=0.1):\n",
    "    x_node, y_node = node.get_position()\n",
    "    x_goal, y_goal = goal.get_position()\n",
    "    return manhattan(node, goal) * (1 + delta)\n",
    "\n",
    "# There are many states with the same f-cost, and we have to choose the order in which to expand them.\n",
    "# For tie_breaking_low_g_cost, we preferred states closer to the start node than the goal state.\n",
    "\n",
    "\n",
    "def tie_breaking_low_g_cost(node, goal, delta=0.1):\n",
    "    x_node, y_node = node.get_position()\n",
    "    x_goal, y_goal = goal.get_position()\n",
    "    return manhattan(node, goal) * (1 - delta)\n",
    "\n",
    "\n",
    "def draw_file_name(current, draw):\n",
    "    while current.parent != None:\n",
    "        current = current.parent\n",
    "        current.color = PURPLE\n",
    "        draw()\n",
    "    return None\n",
    "\n",
    "\n",
    "def make_grid(rows, width):\n",
    "    grid = []\n",
    "    space = width // rows\n",
    "    for i in range(rows):\n",
    "        grid.append([])\n",
    "        for j in range(rows):\n",
    "            node = Node(i, j, space)\n",
    "            grid[i].append(node)\n",
    "    return grid\n",
    "\n",
    "\n",
    "def draw_grid(screen, rows, width):\n",
    "    space = width // rows\n",
    "    for i in range(rows):\n",
    "        pygame.draw.line(screen, GREY, (0, i * space), (width, i * space))\n",
    "        for j in range(rows):\n",
    "            pygame.draw.line(screen, GREY, (j * space, 0), (j * space, width))\n",
    "\n",
    "\n",
    "def draw_main(screen, grid, rows, width):\n",
    "    pygame.event.get()\n",
    "    screen.fill(WHITE)\n",
    "    for row in grid:\n",
    "        for node in row:\n",
    "            node.draw(screen)\n",
    "    draw_grid(screen, rows, width)\n",
    "    pygame.display.update()\n",
    "\n",
    "\n",
    "def Astar(draw, grid, start, goal, heuristic):\n",
    "    step = 0\n",
    "    open_set = queue.PriorityQueue()\n",
    "    open_set.put((0, step, start))\n",
    "    start.g = 0\n",
    "    check = {start}\n",
    "    while not open_set.empty():\n",
    "        step += 1\n",
    "        current = open_set.get()[2]\n",
    "        check.remove(current)\n",
    "        if current.get_position() == goal.get_position():\n",
    "            draw_file_name(goal, draw)\n",
    "            goal.color = TURQUOISE\n",
    "            return True\n",
    "        for neighbor in current.neighbors:\n",
    "            step += 1\n",
    "            tmp = current.g + 1\n",
    "            if tmp < neighbor.g:\n",
    "                neighbor.parent = current\n",
    "                neighbor.g = tmp\n",
    "                neighbor.f = tmp + heuristic(neighbor, goal)\n",
    "                if neighbor not in check:\n",
    "                    open_set.put((neighbor.f, step, neighbor))\n",
    "                    check.add(neighbor)\n",
    "                    neighbor.color = GREEN\n",
    "        draw()\n",
    "        if current != start:\n",
    "            current.color = RED\n",
    "    return False\n",
    "\n",
    "\n",
    "def GreedyBFS(draw, grid, start, goal, heuristic):\n",
    "    step = 0\n",
    "    open_set = queue.PriorityQueue()\n",
    "    open_set.put((0, step, start))\n",
    "    start.g = 0\n",
    "    check = {start}\n",
    "    while not open_set.empty():\n",
    "        step += 1\n",
    "        current = open_set.get()[2]\n",
    "        check.remove(current)\n",
    "        if current.get_position() == goal.get_position():\n",
    "            draw_file_name(goal, draw)\n",
    "            goal.color = TURQUOISE\n",
    "            return True\n",
    "        for neighbor in current.neighbors:\n",
    "            step += 1\n",
    "            tmp = current.g + 1\n",
    "            if tmp < neighbor.g:\n",
    "                neighbor.parent = current\n",
    "                neighbor.g = tmp\n",
    "                neighbor.f = heuristic(neighbor, goal)\n",
    "                if neighbor not in check:\n",
    "                    open_set.put((neighbor.f, step, neighbor))\n",
    "                    check.add(neighbor)\n",
    "                    neighbor.color = GREEN\n",
    "        draw()\n",
    "        if current != start:\n",
    "            current.color = RED\n",
    "    return False\n",
    "\n",
    "\n",
    "def UCS(draw, grid, start, goal, heuristic=None):\n",
    "    step = 0\n",
    "    open_set = queue.PriorityQueue()\n",
    "    open_set.put((0, step, start))\n",
    "    start.g = 0\n",
    "    check = {start}\n",
    "    while not open_set.empty():\n",
    "        step += 1\n",
    "        current = open_set.get()[2]\n",
    "        check.remove(current)\n",
    "        if current.get_position() == goal.get_position():\n",
    "            draw_file_name(goal, draw)\n",
    "            goal.color = TURQUOISE\n",
    "            return True\n",
    "        for neighbor in current.neighbors:\n",
    "            step += 1\n",
    "            tmp = current.g + 1\n",
    "            if tmp < neighbor.g:\n",
    "                neighbor.parent = current\n",
    "                neighbor.g = tmp\n",
    "                neighbor.f = tmp\n",
    "                if neighbor not in check:\n",
    "                    open_set.put((neighbor.f, step, neighbor))\n",
    "                    check.add(neighbor)\n",
    "                    neighbor.color = GREEN\n",
    "        draw()\n",
    "        if current != start:\n",
    "            current.color = RED\n",
    "    return False\n",
    "\n",
    "\n",
    "def main(screen, width, start, goal, gradient, algorithm, heuristic=manhattan):\n",
    "    n = len(img)\n",
    "    grid = make_grid(n, width)\n",
    "    x_start, y_start = start\n",
    "    x_goal, y_goal = goal\n",
    "    run = True\n",
    "\n",
    "    while run:\n",
    "        draw_main(screen, grid, n, width)\n",
    "        start = grid[y_start][x_start]\n",
    "        start.color = YELLOW\n",
    "        goal = grid[y_goal][x_goal]\n",
    "        goal.color = TURQUOISE\n",
    "        for row in grid:\n",
    "            for node in row:\n",
    "                node.get_neighbors(grid, gradient)\n",
    "        result = algorithm(lambda: draw_main(\n",
    "            screen, grid, n, width), grid, start, goal, heuristic)\n",
    "\n",
    "        if result == True:\n",
    "            time.sleep(3)\n",
    "            run = False\n",
    "    pygame.quit()\n",
    "\n",
    "\n",
    "start = (1, 0)\n",
    "goal = (49, 49)\n",
    "gradient = 1  # Take 1 for see the big difference between heuristic functions for low version of Astar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Astar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 600 # Window width\n",
    "\n",
    "screen = pygame.display.set_mode((WIDTH, WIDTH))\n",
    "pygame.display.set_caption(\"Manhattan distance - low version of Astar\")\n",
    "main(screen, WIDTH,start,goal,gradient,Astar,manhattan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen = pygame.display.set_mode((WIDTH, WIDTH))\n",
    "pygame.display.set_caption(\"Tie-breaking High g-cost - low version of Astar\")\n",
    "main(screen, WIDTH,start,goal,gradient,Astar,tie_breaking_high_g_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen = pygame.display.set_mode((WIDTH, WIDTH))\n",
    "pygame.display.set_caption(\"Tie-breaking Low g-cost - low version of Astar\")\n",
    "\n",
    "main(screen, WIDTH,start,goal,gradient,Astar,tie_breaking_low_g_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that, the number of steps for this visualization is significant, the reason is that in this code, we try the old version of A*, which is not efficient as our algorithm described by pseudo-code in our report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Greedy BFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen = pygame.display.set_mode((WIDTH, WIDTH))\n",
    "pygame.display.set_caption(\"GreedyBFS\")\n",
    "\n",
    "main(screen, WIDTH,start,goal,gradient,GreedyBFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. UCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen = pygame.display.set_mode((WIDTH, WIDTH))\n",
    "pygame.display.set_caption(\"UCS\")\n",
    "\n",
    "main(screen, WIDTH,start,goal,gradient,UCS,manhattan)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Capstone Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
